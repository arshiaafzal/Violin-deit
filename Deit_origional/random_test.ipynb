{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from timm.models.vision_transformer import VisionTransformer, _cfg, Mlp, HybridEmbed, PatchEmbed\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "\n",
    "from curves import compute_curve_order, coords_to_index, index_to_coords_indexes\n",
    "import numpy as np\n",
    "\n",
    "class Violin_Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., \n",
    "                 pos_emb = True, cls_tok = True, curve_list = ['s', 'sr', 'h', 'hr', 'm', 'mr','z', 'zr'],num_patches=196, qk_norm=False,mask='learned',scale=False,method='mul_v1',initialize=False,mask_sum='weighted'):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.cls_tok = cls_tok\n",
    "        self.curve_list = curve_list\n",
    "        self.qk_norm = qk_norm\n",
    "        self.mask = mask\n",
    "        self.scale = scale\n",
    "        self.method = method\n",
    "\n",
    "        self.curve_indices_inv = []\n",
    "        self.ai_list = []\n",
    "\n",
    "        N = num_patches \n",
    "        order = torch.range(0,N-1)\n",
    "        S = int(np.sqrt(N))\n",
    "        grid = order.view(S,S).clone()\n",
    "\n",
    "        # for curve in curve_list:\n",
    "        #     if curve not in ['s', 'sr', 'h', 'hr', 'm', 'mr', 'z', 'zr']:\n",
    "        #         raise ValueError(\"Invalid value for curve. Allowed values are: 's', 'sr', 'h', 'hr', 'm', 'mr', 'z', 'zr'.\")\n",
    "            \n",
    "        #     curve_coords = compute_curve_order(grid, curve)\n",
    "        #     self.curve_indices_inv.append(torch.tensor(index_to_coords_indexes(curve_coords, S,S)  , dtype=torch.long ))  \n",
    "        #     if mask == 'fixed':\n",
    "        #         self.ai_list.append(torch.ones(num_heads) * 0.996)\n",
    "        #     else:\n",
    "        self.ai_list = nn.ParameterList([nn.Parameter(torch.randn(num_heads)) for _ in range(len(curve_list))])\n",
    "        # self.ai_list.append(nn.Parameter(torch.randn(num_heads)))\n",
    "\n",
    "        if qk_norm:\n",
    "            self.q_norm = nn.LayerNorm(head_dim)\n",
    "            self.k_norm = nn.LayerNorm(head_dim)\n",
    "\n",
    "        if mask == 'weighted':\n",
    "            self.mask_weights = nn.Parameter(torch.randn(len(self.ai_list)))\n",
    "        else:\n",
    "            self.mask_weights =torch.ones(len(self.ai_list)) / len(self.ai_list)\n",
    "\n",
    "        if scale:\n",
    "            self.normalize = nn.Parameter(torch.randn(num_heads))\n",
    "        else:\n",
    "            self.normalize = torch.ones(num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110410/974834263.py:37: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  order = torch.range(0,N-1)\n"
     ]
    }
   ],
   "source": [
    "a = Violin_Attention(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([192, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n",
      "<class 'torch.Tensor'> torch.Size([8])\n",
      "<class 'torch.Tensor'> torch.Size([8])\n",
      "<class 'torch.Tensor'> torch.Size([8])\n",
      "<class 'torch.Tensor'> torch.Size([8])\n",
      "<class 'torch.Tensor'> torch.Size([8])\n",
      "<class 'torch.Tensor'> torch.Size([8])\n",
      "<class 'torch.Tensor'> torch.Size([8])\n",
      "<class 'torch.Tensor'> torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for param in a.parameters():\n",
    "    print(type(param.data), param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9933)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(torch.tensor(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "novavit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
